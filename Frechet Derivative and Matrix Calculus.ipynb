{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e642c46-f72a-4b8a-9820-d9e20be0737d",
   "metadata": {},
   "source": [
    "# Fr&#233;chet Derivative and Matrix Calculus\n",
    "This article introduces Fr&#233;chet derivative, a perspective towards derivatives and gradients that enables efficient computation of gradients w.r.t. matrices which is especially useful for deriving back propagation rules for neural networks in matrix form. This article then takes multilayer perceptron and batch normalization as examples and derives the gradients in matrix form, followed by corresponding code implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b70aa-9344-465b-8f0d-3993a15c64f6",
   "metadata": {},
   "source": [
    "## From Limits to Fr&#233;chet Derivative\n",
    "For real-valued function $f: \\mathbb{R} \\to \\mathbb{R}$, if the derivative at point $x$ exists, it is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f'(x) = \\lim_{\\epsilon \\to 0}\\frac{f(x + \\epsilon) - f(x)}{\\epsilon}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "While this definition generalizes to vector- and matrix-valued functions, and is often related with the slope of $f$ at some point of interest, the concept of derivative or gradient could be viewed from another perspective, which is to approximate the difference in function value with a term that is linear w.r.t. the difference in the variable:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(x + \\epsilon) = f(x) + f'(x)\\epsilon + o(\\epsilon)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $o(\\epsilon)$ is a term that shrinks much faster than $\\epsilon$. Technically, it refers to some term $g(\\epsilon)$ that satisfies\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\lim_{\\epsilon \\to 0}\\frac{g(\\epsilon)}{\\epsilon} = 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This view of derivative is called the Fr&#233;chet derivative. The implication of which on finding the derivative at a point is that if we could manage to separate a linear term from the difference in the function value while the residual is of $o(\\epsilon)$, then the separated weights is exactly the derivative that we are looking for. Now that we've grasped the idea of the Fr&#233;chet derivative by looking at real-valued functions, we'll move on to see it's generalized form to vector- and matrix-valued functions, which is achieved via the inner product.\n",
    "\n",
    "For vector-valued function $f: \\mathbb{R}^n \\to \\mathbb{R}$, the linear term is computed as the vector inner-product between the gradient and the differences in the variables. Each variable is assigned with a partial derivative, and the differences in all variables are aggregated to get the approximation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(\\mathbf{x} + \\boldsymbol\\epsilon)\n",
    "    &= f(\\mathbf{x})\n",
    "        + \\langle{\\nabla_\\mathbf{x}(f), \\boldsymbol\\epsilon}\\rangle\n",
    "        + o(\\lVert{\\boldsymbol\\epsilon}\\rVert) \\\\\n",
    "    &= f(\\mathbf{x})\n",
    "        + {\\nabla_\\mathbf{x}(f)}^\\top\\boldsymbol\\epsilon\n",
    "        + o(\\lVert{\\boldsymbol\\epsilon}\\rVert).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For matrix-valued function $f: \\mathbb{R}^{m \\times n} \\to \\mathbb{R}$, the linear term is computed as the matrix inner-product, which equals to the vector inner product after we vectorize the two matrices, or the sum of the elements after a Hadamard product, but most importantly, the trace of the matrix computed by multiplying the two matrices with one of them transposed. The idea here is essentially the same as for vector-valued functions in terms that we aggregate the differences in all variables:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(\\mathbf{X} + \\boldsymbol{\\mathcal{E}})\n",
    "    &= f(\\mathbf{X})\n",
    "        + \\langle{\\nabla_\\mathbf{X}(f), \\boldsymbol{\\mathcal{E}}}\\rangle\n",
    "        + o(\\lVert{\\boldsymbol{\\mathcal{E}}}\\rVert_F) \\\\\n",
    "    &= f(\\mathbf{X})\n",
    "        + \\langle{\\operatorname{vec}(\\nabla_\\mathbf{X}(f)), \\operatorname{vec}(\\boldsymbol{\\mathcal{E}})}\\rangle\n",
    "        + o(\\lVert{\\boldsymbol{\\mathcal{E}}}\\rVert_F) \\\\\n",
    "    &= f(\\mathbf{X})\n",
    "        + \\mathbf{1}^\\top\\cdot(\\nabla_\\mathbf{X}(f)\\odot\\boldsymbol{\\mathcal{E}})\\cdot\\mathbf{1}\n",
    "        + o(\\lVert{\\boldsymbol{\\mathcal{E}}}\\rVert_F) \\\\\n",
    "    &= f(\\mathbf{X})\n",
    "        + \\operatorname{tr}({\\nabla_\\mathbf{X}(f)}^\\top\\boldsymbol{\\mathcal{E}})\n",
    "        + o(\\lVert{\\boldsymbol{\\mathcal{E}}}\\rVert_F)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\lVert{\\boldsymbol{\\mathcal{E}}}\\rVert_F = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}\\mathbf{A}_{i,j}^2} = \\sqrt{\\operatorname{tr}({\\mathbf{A}^\\top\\mathbf{A}}})$ is the Frobenius norm. The reason that trace is preferred here is because it offers a range of convenient manipulations which are useful to separate out the difference part to get the gradient. We'll see how it works out in the examples. But before that, we need some manipulations of the differential form to properly attribute the difference in loss function to the variables that we would like to compute gradients w.r.t."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364fa43",
   "metadata": {},
   "source": [
    "## Important Manipulations\n",
    "### Manipulations of the Differential Form\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    d(\\mathbf{X}\\mathbf{Y}) = d(\\mathbf{X})\\mathbf{Y} + \\mathbf{X}d(\\mathbf{Y})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Manipulations of the $\\textit{trace}$ Operation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\operatorname{tr}(\\mathbf{X}\\mathbf{Y}) = \\operatorname{tr}(\\mathbf{Y}\\mathbf{X})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$Proof$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{LHS}\n",
    "    = \\sum_i\\sum_j\\mathbf{X}_{i,j}(\\mathbf{Y}_{j,i}\\mathbf{Z}_{j,i})\n",
    "    = \\sum_i\\sum_j(\\mathbf{X}_{i,j}\\mathbf{Y}^\\top_{i,j})\\mathbf{Z}_{j,i}\n",
    "    = \\text{RHS}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\operatorname{tr}(\\mathbf{X}(\\mathbf{Y}\\odot\\mathbf{Z})) = \\operatorname{tr}((\\mathbf{X}\\odot\\mathbf{Y}^\\top)\\mathbf{Z})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$Proof$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{LHS}\n",
    "    = \\sum_i\\sum_j\\mathbf{X}_{i,j}(\\mathbf{Y}_{j,i}\\mathbf{Z}_{j,i})\n",
    "    = \\sum_i\\sum_j(\\mathbf{X}_{i,j}\\mathbf{Y}^\\top_{i,j})\\mathbf{Z}_{j,i}\n",
    "    = \\text{RHS}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since neural networks consists of iterating linearity and non-linearity, in backpropagation, $\\mathbf{X}$ would be the upstream gradient, $\\mathbf{Y}$ would be the gradient of the activation function (since it is applied element-wise), and $\\mathbf{Z}$ would be a differential form that entails the variable of interest. This manipulation allows for separating the differential form from the Hadamard product into a multiplicative term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a3065",
   "metadata": {},
   "source": [
    "## Examples\n",
    "### Multilayer Perceptron\n",
    "At the $l$-th layer of a multilayer perceptron, the latent representations $\\mathbf{H}^{(l - 1)} \\in \\mathbb{R}^{N \\times d^{(l - 1)}}$ from the $(l - 1)$-th layer pass through a linear transformation $\\mathbf{W}^{(l)} \\in \\mathbb{R}^{d^{(l - 1)} \\times d^{(l)}}$, followed by a nonlinear activation function $\\phi: \\mathbb{R} \\to \\mathbb{R}$ which is applied element-wise, to get the updated representations $\\mathbf{H}^{(l)} \\in \\mathbb{R}^{N \\times d^{(l)}}$. We omit the bias term here since it could be easily incorporated by appending a column vector of $1$s to $\\mathbf{H}^{(l - 1)}$. Therefore, The forward computation is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{H}^{(l)} = \\phi(\\mathbf{H}^{(l - 1)}\\mathbf{W}^{(l)}).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Suppose we've already computed the upstream gradient $\\nabla_{\\mathbf{H}^{(l)}}(\\mathcal{J})$. To proceed on backpropagation with the chain rule we should now compute the local gradient $\\nabla_{\\mathbf{H}^{(l)}}(\\mathbf{H}^{(l - 1)})$, which is undefined since it's matrix-to-matrix. However, we can derive it without trouble with the Fr&#233;chet derivative as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    d(\\mathcal{J})\n",
    "    &= \\operatorname{tr}(\n",
    "        {\\nabla_{\\mathbf{H}^{(l)}}(\\mathcal{J})}^\\top\n",
    "        d(\\mathbf{H}^{(l)})\n",
    "    ) \\\\\n",
    "    &= \\operatorname{tr}(\n",
    "        {\\nabla_{\\mathbf{H}^{(l)}}(\\mathcal{J})}^\\top\n",
    "        d(\\phi(\\mathbf{H}^{(l - 1)}\\mathbf{W}^{(l)}))\n",
    "    ) \\\\\n",
    "    &= \\operatorname{tr}(\n",
    "        {\\nabla_{\\mathbf{H}^{(l)}}(\\mathcal{J})}^\\top\n",
    "        (\n",
    "            \\phi'(\\mathbf{H}^{(l - 1)}\\mathbf{W}^{(l)})\n",
    "            \\odot d(\\mathbf{H}^{(l - 1)}\\mathbf{W}^{(l)})\n",
    "        )\n",
    "    ) \\\\\n",
    "    &= \\operatorname{tr}(\n",
    "        (\n",
    "            {\\nabla_{\\mathbf{H}^{(l)}}(\\mathcal{J})}^\\top\n",
    "            \\odot {\\phi'(\\mathbf{H}^{(l - 1)}\\mathbf{W}^{(l)})}^\\top\n",
    "        )\n",
    "        d(\\mathbf{H}^{(l - 1)}\\mathbf{W}^{(l)})\n",
    "    ).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here $\\phi'(\\mathbf{H}^{(l - 1)}\\mathbf{W}^{(l)})$ is evaluated element-wise. To compute $\\nabla_\\mathcal{J}(\\mathbf{W}^{(l)})$, we separate out $d(\\mathbf{W}^{(l)})$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & d(\\mathcal{J}) = \\operatorname{tr}(\n",
    "        (\n",
    "            {\\nabla_{\\mathbf{H}^{(l)}}(\\mathcal{J})}^\\top\n",
    "            \\odot {\\phi'(\\mathbf{H}^{(l - 1)}\\mathbf{W}^{(l)})}^\\top\n",
    "        )\n",
    "        \\mathbf{H}^{(l - 1)}d(\\mathbf{W}^{(l)})\n",
    "    ) \\\\\n",
    "    \\Rightarrow\n",
    "    & \\nabla_\\mathcal{J}(\\mathbf{W}^{(l)}) = (\n",
    "        (\n",
    "            {\\nabla_{\\mathbf{H}^{(l)}}(\\mathcal{J})}^\\top\n",
    "            \\odot {\\phi'(\\mathbf{H}^{(l - 1)}\\mathbf{W}^{(l)})}^\\top\n",
    "        )\n",
    "        \\mathbf{H}^{(l - 1)}\n",
    "    )^\\top \\\\\n",
    "    \\Rightarrow\n",
    "    & \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\, = {\\mathbf{H}^{(l - 1)}}^\\top(\n",
    "        \\nabla_{\\mathbf{H}^{(l)}}(\\mathcal{J})\n",
    "        \\odot \\phi'(\\mathbf{H}^{(l - 1)}\\mathbf{W}^{(l)})\n",
    "    )\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
