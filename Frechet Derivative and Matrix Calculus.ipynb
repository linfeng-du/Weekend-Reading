{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e642c46-f72a-4b8a-9820-d9e20be0737d",
   "metadata": {},
   "source": [
    "# Fr&#233;chet Derivative and Matrix Calculus\n",
    "This article introduces Fr&#233;chet derivative, a new perspective towards derivatives and gradients that also enables efficient computation of gradients w.r.t. matrices which is especially useful for deriving back propagation rules for neural networks in matrix form. Later parts of this article takes batch normalization as an example and computes the gradients in matrix form, followed by a corresponding code implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b70aa-9344-465b-8f0d-3993a15c64f6",
   "metadata": {},
   "source": [
    "## From Limits to Fr&#233;chet Derivative\n",
    "For real-valued function $f: \\mathbb{R} \\to \\mathbb{R}$, if the derivative at point $x$ exists, it is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f'(x) = \\lim_{\\epsilon \\to 0}\\frac{f(x+\\epsilon) - f(x)}{\\epsilon}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "While this definition generalizes to vector- and matrix-valued functions, and is often related with the slope of $f$ at some point of interest, the concept of derivative or gradient could be viewed from another perspective, which is to approximate the difference in function value with a term that is linear w.r.t. the difference in the variable:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(x+\\epsilon) = f(x) + f'(x)\\epsilon + o(\\epsilon)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $o(\\epsilon)$ is a term that shrinks much faster than $\\epsilon$. Technically, it refers to some term $g(\\epsilon)$ that satisfies\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\lim_{\\epsilon \\to 0}\\frac{g(\\epsilon)}{\\epsilon} = 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This view of derivative is called the Fr&#233;chet derivative. The implication of which on finding the derivative at a point is that if we could manage to separate a linear term from the difference in the function value while the residual is of $o(\\epsilon)$, then the separated weights is exactly the derivative at this point. For neural networks, the function that we would like to take gradient of is often linear w.r.t. the variable (since non-linearity is always applied element-wise, we could easily take care of them separately using the chain rule). In this case, the $o(\\epsilon)$ term would become 0, which further eases the derivation. Now that we've grasped the idea of the Fr&#233;chet derivative by looking at real-valued functions, we'll move on to see the generalized form to vector- and matrix-valued functions, which is achieved via the inner product.\n",
    "\n",
    "For vector-valued function $f: \\mathbb{R^n} \\to \\mathbb{R}$, the linear term is computed as the vector inner-product between the gradient and the difference in the variables:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(\\mathbf{x}+\\boldsymbol\\epsilon)\n",
    "    &= f(\\mathbf{x}) + \\langle\\nabla_\\mathbf{x}f, \\boldsymbol\\epsilon\\rangle + o(\\lVert\\boldsymbol\\epsilon\\rVert) \\\\\n",
    "    &= f(\\mathbf{x}) + (\\nabla_\\mathbf{x}f)^\\top\\boldsymbol\\epsilon + o(\\lVert\\boldsymbol\\epsilon\\rVert).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Each dimension here is assigned with a partial derivative, and the effects in the dimensions are summed to get the approximation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
